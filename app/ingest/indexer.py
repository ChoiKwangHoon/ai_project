"""
indexer.py : PDF â†’ ì²­í‚¹ â†’ ì„ë² ë”© ìƒì„± â†’ Azure Cognitive Search ì—…ë¡œë“œ
- Portal Indexer ëŒ€ì‹  ì§ì ‘ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ë²¡í„° í•„ë“œ(text_vector)ì— ì €ì¥
"""

import os
import uuid
import logging
from typing import List

from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient, IndexDocumentsBatch


from openai import AzureOpenAI
from app.config import AppConfig

# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (PyPDF2 ì‚¬ìš©, fitz ì œê±°)
from PyPDF2 import PdfReader

# ë¡œê±° ì„¤ì •
logger = logging.getLogger("entraaid_app")
logging.basicConfig(level=logging.INFO)


def load_pdf_text(pdf_path: str) -> List[str]:
    """
    PDF íŒŒì¼ì—ì„œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    """
    reader = PdfReader(pdf_path)
    texts = []
    for i, page in enumerate(reader.pages):
        text = page.extract_text()
        if text:
            texts.append(text.strip())
        else:
            logger.warning(f"âš ï¸ {i+1} í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    logger.info(f"âœ… PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: ì´ {len(texts)} í˜ì´ì§€")
    return texts


def chunk_texts(texts: List[str], max_chunk_size: int = 1000) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì¼ì • ê¸¸ì´(max_chunk_size) ë‹¨ìœ„ë¡œ ì²­í‚¹
    """
    chunks = []
    for text in texts:
        while len(text) > max_chunk_size:
            chunks.append(text[:max_chunk_size])
            text = text[max_chunk_size:]
        if text:
            chunks.append(text)
    logger.info(f"âœ… ì²­í‚¹ ì™„ë£Œ: ì´ {len(chunks)} ì²­í¬ ìƒì„±")
    return chunks


def embed_texts(chunks: List[str]) -> List[List[float]]:
    """
    Azure OpenAI Embedding API í˜¸ì¶œ
    """
    client = AzureOpenAI(
        api_key=AppConfig.AOAI_API_KEY,
        api_version=AppConfig.AOAI_API_VERSION,
        azure_endpoint=AppConfig.AOAI_ENDPOINT,
    )

    response = client.embeddings.create(
        model=AppConfig.AOAI_EMBED_DEPLOYMENT,  # .envì— ì§€ì •í•œ ì„ë² ë”© ëª¨ë¸
        input=chunks,
    )

    embeddings = [d.embedding for d in response.data]
    logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: ì´ {len(embeddings)} ë²¡í„°")
    return embeddings


def upload_to_search(chunks: List[str], embeddings: List[List[float]], pdf_name: str):
    """
    ì²­í¬ + ì„ë² ë”©ì„ Azure Cognitive Searchì— ì—…ë¡œë“œ
    """
    search_client = SearchClient(
        endpoint=AppConfig.AIS_ENDPOINT,
        index_name=AppConfig.AIS_INDEX,
        credential=AzureKeyCredential(AppConfig.AIS_API_KEY),
    )

    batch = IndexDocumentsBatch()

    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        doc = {
            "chunk_id": f"{pdf_name}_{i}",  # í‚¤: í—ˆìš© ë¬¸ìë§Œ ì‚¬ìš©
            "parent_id": pdf_name,
            "chunk": chunk,
            "title": pdf_name,
            "text_vector": embedding,
        }
        batch = IndexDocumentsBatch()
        batch.add_upload_actions([doc])  # âœ… ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬


    result = search_client.index_documents(batch=batch)
    logger.info(f"âœ… {len(chunks)}ê°œ ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ")
    print("âœ… ì—…ë¡œë“œ ê²°ê³¼:", result)


def index_pdf(pdf_path: str):
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    """
    try:
        logger.info(f"ğŸ“„ PDF íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]

        texts = load_pdf_text(pdf_path)
        chunks = chunk_texts(texts)
        embeddings = embed_texts(chunks)
        upload_to_search(chunks, embeddings, pdf_name)

        logger.info("ğŸ‰ ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")

    except Exception as e:
        logger.error(f"âŒ ì¸ë±ì‹± ì‹¤íŒ¨: {e}", exc_info=True)
        print(f"âŒ ì¸ë±ì‹± ì‹¤íŒ¨: {e}")


# ë‹¨ë… ì‹¤í–‰ ì‹œ ë™ì‘
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="PDF íŒŒì¼ ì¸ë±ì‹±")
    parser.add_argument("file_path", type=str, help="ì¸ë±ì‹±í•  PDF íŒŒì¼ ê²½ë¡œ")
    args = parser.parse_args()

    index_pdf(args.file_path)
